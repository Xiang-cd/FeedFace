autoencoder:
  pretrained_path: models/autoencoder_kl.pth
batch_size: 16
caption_decoder:
  hidden_dim: 64
  pretrained_path: models/caption_decoder.pth
  tokenizer_path: models/gpt2
clip_img_dim: 512
clip_img_model: ViT-B/32
clip_text_dim: 768
clip_text_model: openai/clip-vit-large-patch14
config_name: competition_final
data_name: PeopleDataset
data_type: 1
dataset:
  detect_resolution: 224
  jsonl_files:
  - /mnt/vepfs/xcd-share/dataset/FFHQ_in_the_wild-llava_1_5_13b.jsonl
  name: PeopleDataset
  repeat: 10
detect_resolution: 224
eval_interval: 100000
feed_model:
  block_args:
    block_para:
      d_head: 24
      dim: 1536
      n_heads: 64
      zero_linear: true
    name: SDBT
    num_block: 1
  block_names:
  - in_blocks.0
  - in_blocks.2
  - in_blocks.4
  - in_blocks.6
  - in_blocks.8
  - in_blocks.10
  - in_blocks.12
  - out_blocks.0
  - out_blocks.2
  - out_blocks.4
  - out_blocks.6
  - out_blocks.8
  - out_blocks.10
  - out_blocks.12
  multiplier: 1.0
  name: t2iadp1_no_encode
  preprocess_module: CLIPfrozen2
  type_: 2
  verbose: true
gradient_accumulation_steps: 1
hparams: default
log_interval: 5000
loss: t2iadp1_no_encode_addition
lr_scheduler:
  name: customized
  warmup_steps: 100
mode: t2i
n_iter: 1
n_samples: 3
nnet:
  attn_drop_rate: 0.0
  clip_img_dim: 512
  depth: 30
  drop_rate: 0.0
  embed_dim: 1536
  img_size: 64
  in_chans: 4
  mlp_ratio: 4
  mlp_time_embed: false
  name: UViTCondToken
  num_heads: 24
  num_text_tokens: 77
  patch_size: 2
  pos_drop_rate: 0.0
  qkv_bias: false
  text_dim: 64
  use_checkpoint: true
nnet_path: models/uvit_v1.pth
nrow: 4
pred: noise_pred
sample:
  sample_steps: 30
  scale: 7.0
  t2i_cfg_mode: true_uncond
seed: 1234
text_dim: 64
z_shape:
- 4
- 64
- 64
